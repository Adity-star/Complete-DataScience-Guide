{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate\n"
      ],
      "metadata": {
        "id": "3DkxKcNdw1i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets\n",
        "! pip install nltk\n",
        "! pip install rouge_score"
      ],
      "metadata": {
        "id": "kUXrWU48xJz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "W6daVUDNyRPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate>=0.26.0 --upgrade"
      ],
      "metadata": {
        "id": "k42j8276yZRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "EgfowjQU2B0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUp3vhYLvhS2"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline,set_seed\n",
        "from datasets import load_dataset,load_from_disk\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYxAbxQpvhS3"
      },
      "source": [
        "\n",
        "This code checks if a GPU is available for use with PyTorch. If a GPU is available, it sets the device to `\"cuda\"`, otherwise, it uses the CPU (`\"cpu\"`). It then prints whether CUDA (GPU support) is available as a Boolean value.\n",
        "\n",
        "### Code:\n",
        "\n",
        "```python\n",
        "# Check if CUDA (GPU support) is available and select the appropriate device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Print if CUDA is available (True or False)\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xk7aAOovhS5"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-tHTEpdvhS5"
      },
      "source": [
        "\n",
        "This code demonstrates how to load a pre-trained sequence-to-sequence model and tokenizer from the Hugging Face `transformers` library. It first checks if a GPU (CUDA) is available and selects the device accordingly. Then, it loads the tokenizer and model (`google/pegasus-cnn_dailymail`) onto the selected device.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prbE5IRMvhS6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "model_pegas = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSCxZeZtvhS6"
      },
      "source": [
        "\n",
        "This code downloads a zip file (`summarizer-data.zip`) from a GitHub repository using `wget` and then extracts the contents of the zip file using the `unzip` command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX_80N4HvhS6"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
        "!unzip summarizer-data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfzlLMQBvhS7"
      },
      "source": [
        "### This code unzips the file and extract the train,test and validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JExV4kSvhS8"
      },
      "source": [
        "* load the data from the disk (dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y715LG7vhS7"
      },
      "outputs": [],
      "source": [
        "dataset_samsum = load_from_disk('samsum_dataset')\n",
        "dataset_samsum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxuYARSwvhS9"
      },
      "source": [
        "\n",
        "This code calculates and prints the lengths of different splits in a dataset, followed by displaying the feature names in the training set. It also prints a sample dialogue from the test set and the corresponding summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKWJBdGrvhS9"
      },
      "outputs": [],
      "source": [
        "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n",
        "print(f\"Split lengths: {split_lengths}\")\n",
        "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
        "print(\"\\nDialogue:\")\n",
        "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
        "\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "\n",
        "print(dataset_samsum[\"test\"][1][\"summary\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JbSkF-qvhS9"
      },
      "source": [
        "\n",
        "This function, `convert_examples_to_features`, takes a batch of examples and converts the dialogue and summary into tokenized input and target sequences. It uses the Hugging Face tokenizer to encode the input (dialogue) and target (summary), truncating them to a maximum length. The function then returns a dictionary containing the tokenized input IDs, attention mask, and target labels.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQzqZ4bdvhS9"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
        "\n",
        "    return {\n",
        "        'input_ids' : input_encodings['input_ids'],\n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'labels': target_encodings['input_ids']\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GJoa3H-vhS9"
      },
      "source": [
        "### We apply our convert_example_to_feature to our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1GL891BvhS-"
      },
      "outputs": [],
      "source": [
        "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5rA5VifvhS-"
      },
      "outputs": [],
      "source": [
        "dataset_samsum_pt[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbRF_CtEvhS-"
      },
      "source": [
        "\n",
        "This code snippet imports `DataCollatorForSeq2Seq` from the Hugging Face `transformers` library and creates a `seq2seq_data_collator` object. This data collator is used to dynamically pad the input sequences to the correct length during training, ensuring that the input and target sequences are aligned properly for the model. It uses the previously defined tokenizer and model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BWYMnHavhS-"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model = model_pegas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_LGZdruvhS-"
      },
      "source": [
        "\n",
        "This code sets up the training parameters for the model using `TrainingArguments`. Below is an explanation of each parameter:\n",
        "\n",
        "### Parameters:\n",
        "\n",
        "- **`output_dir`**:  \n",
        "  Directory where the model checkpoints, logs, and evaluation results will be saved. In this case, it's set to `'pegas-samsum'`.\n",
        "\n",
        "- **`num_train_epochs`**:  \n",
        "  Specifies the number of times the entire dataset will be passed through the model during training. Here it is set to 1, meaning the model will train for 1 epoch.\n",
        "\n",
        "- **`warmup_steps`**:  \n",
        "  The number of steps to perform learning rate warmup. During this phase, the learning rate will gradually increase. It is set to 500 steps in this case.\n",
        "\n",
        "- **`per_device_train_batch_size`**:  \n",
        "  Batch size for training on each device (GPU or CPU). In this case, it is set to 1, which means the model will train on one sample at a time per device.\n",
        "\n",
        "- **`per_device_eval_batch_size`**:  \n",
        "  Batch size for evaluation during validation. Here, it's set to 1, meaning evaluation will happen with one sample per device.\n",
        "\n",
        "- **`weight_decay`**:  \n",
        "  The strength of weight decay regularization used to prevent overfitting by penalizing large weights. It is set to 0.01, meaning the model will have a small penalty on large weights.\n",
        "\n",
        "- **`logging_steps`**:  \n",
        "  Defines how often (in steps) the logs are recorded. In this case, it is set to 10, meaning logging will happen every 10 training steps.\n",
        "\n",
        "- **`evaluation_strategy`**:  \n",
        "  Specifies when to run evaluation during training. In this case, it is set to `'steps'`, meaning evaluation will happen after a specified number of steps (`eval_steps`).\n",
        "\n",
        "- **`eval_steps`**:  \n",
        "  Defines the number of steps between evaluations during training. Here it is set to 500, so evaluation will occur every 500 steps.\n",
        "\n",
        "- **`save_steps`**:  \n",
        "  Specifies how often the model is saved during training. A value of `1e6` is used here, which essentially means the model will only be saved after a very large number of steps (effectively disabling frequent saves).\n",
        "\n",
        "- **`gradient_accumulation_steps`**:  \n",
        "  The number of steps over which gradients are accumulated before the model weights are updated. It is set to 16, meaning gradients are accumulated over 16 steps before performing a weight update. This can help with training efficiency on smaller batch sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoR6CXX-vhS-"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir = 'pegas-samsum',\n",
        "    num_train_epochs = 1,\n",
        "    warmup_steps = 500,\n",
        "    per_device_train_batch_size = 1,\n",
        "    per_device_eval_batch_size = 1,\n",
        "    weight_decay = 0.01,\n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'steps',\n",
        "    eval_steps = 500,\n",
        "    save_steps = 1e6,\n",
        "    gradient_accumulation_steps = 16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T4iEg8VvhS-"
      },
      "source": [
        "\n",
        "This code initializes a `Trainer` object from the Hugging Face `transformers` library. The `Trainer` handles the training loop and evaluation process for the model. Below is an explanation of each parameter used in this setup.\n",
        "\n",
        "### Parameters:\n",
        "\n",
        "- **`model`**:  \n",
        "  The model to be trained. In this case, it is the `model_pegas`, which is the pre-trained `pegasus` model loaded earlier.\n",
        "\n",
        "- **`args`**:  \n",
        "  The training arguments defined using the `TrainingArguments` class. These arguments include settings like batch size, number of epochs, evaluation strategy, and more. Here, `trainer_args` contains all these settings.\n",
        "\n",
        "- **`tokenizer`**:  \n",
        "  The tokenizer used to process input and target sequences during training. It is set to the `tokenizer` object, which is responsible for converting text to token IDs and vice versa.\n",
        "\n",
        "- **`data_collator`**:  \n",
        "  A data collator that handles dynamic padding and batching during training. It is set to `seq2seq_data_collator`, which ensures that the input and output sequences are padded correctly for the sequence-to-sequence task.\n",
        "\n",
        "- **`train_dataset`**:  \n",
        "  The dataset used for training the model. In this case, it's set to the `\"test\"` split of `dataset_samsum_pt`. This might be a typo, as typically the `\"train\"` split would be used here instead of `\"test\"`. Make sure to use the correct split for training.\n",
        "\n",
        "- **`eval_dataset`**:  \n",
        "  The dataset used for evaluating the model during training. It is set to the `\"validation\"` split of `dataset_samsum_pt`, which will be used to monitor the model's performance after each evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCPSI-isvhS-"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model = model_pegas,\n",
        "    args = trainer_args,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = seq2seq_data_collator,\n",
        "    train_dataset = dataset_samsum_pt[\"test\"],\n",
        "    eval_dataset = dataset_samsum_pt[\"validation\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BcP4GCqvhS_"
      },
      "source": [
        "#### Train our data(it takes little time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9ZLaVBJvhS_"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNpPNpthvhS_"
      },
      "source": [
        "\n",
        "The function `generate_batch_sized_chunks` is used to split a large list into smaller, more manageable batches. This is particularly useful for processing large datasets in smaller chunks, such as when performing batch processing in machine learning tasks\n",
        "- **`list_of_elements`**:  \n",
        "  This is the list (or iterable) that you want to split into batches. It could represent any collection of data such as a list of input samples or data points.\n",
        "\n",
        "- **`batch_size`**:  \n",
        "  This is the size of each batch. The function will divide the `list_of_elements` into smaller groups, each containing up to `batch_size` elements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOT2nvC8vhS_"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Z3mvVEvhS_"
      },
      "source": [
        "\n",
        "The function `calculate_metric_on_test_ds` evaluates a model's performance on a test dataset by generating summaries for each article in the dataset, comparing them with the actual summaries, and then calculating a specific metric (such as ROUGE scores). It splits the dataset into smaller batches, processes each batch, generates summaries, and compares them to the reference summaries.\n",
        "\n",
        "- **Batching**: The dataset is split into batches to process the articles and summaries in smaller chunks. This helps with memory efficiency, especially when dealing with large datasets.\n",
        "- **Summary Generation**: The model generates summaries for the articles using beam search and length penalties to ensure the summaries are of appropriate length and quality.\n",
        "- **Metric Calculation**: The function adds the generated summaries and reference summaries to the metric, which computes the desired evaluation score (like ROUGE).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdPgtL1kvhTA"
      },
      "outputs": [],
      "source": [
        "def calculate_metric_on_test_ds(dataset,\n",
        "                                metric,\n",
        "                                model,\n",
        "                                tokenizer,\n",
        "                                batch_size=16,\n",
        "                                device=device,\n",
        "                                column_text=\"article\",\n",
        "                                column_summary=\"highlights\"):\n",
        "\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
        "                        padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
        "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
        "\n",
        "        # Finally, we decode the generated texts,\n",
        "        # replace the  token, and add the decoded texts with the references to the metric.\n",
        "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                clean_up_tokenization_spaces=True)\n",
        "               for s in summaries]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "\n",
        "\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    #  Finally compute and return the ROUGE scores.\n",
        "    score = metric.compute()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ppObWTCvhTA"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "rouge_metric = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwbsM2puvhTA"
      },
      "outputs": [],
      "source": [
        "\n",
        "score = calculate_metric_on_test_ds(\n",
        "    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
        ")\n",
        "\n",
        "rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n",
        "pd.DataFrame(rouge_dict, index = [f'pegasus'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQDFWja8vhTB"
      },
      "source": [
        "### Save our pretrained model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT3JoB1dvhTB"
      },
      "outputs": [],
      "source": [
        "model_pegas.save_pretrained(\"pegasus-samsum-model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PL2AZhkvhTB"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ylX1f7-vhTB"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r35D-7uQvhTB"
      },
      "source": [
        "#### Predictions using our saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZavygRXTvhTB"
      },
      "outputs": [],
      "source": [
        "#Prediction\n",
        "\n",
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
        "\n",
        "\n",
        "\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n",
        "\n",
        "##\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKFmTg_lvhTB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}